{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9c38d43",
   "metadata": {},
   "source": [
    "# Ensemble Inference\n",
    "This code is based on the conference paper: Niimi, J. (2025) \"A Simple Ensemble Strategy for LLM Inference: Towards More Stable Text Classification\" In Proceedings of the 30th International Conference on Natural Language & Information Systems (NLDB 2025). Please refer to GitHub repository [jniimi/ensemble_inference](https://github.com/jniimi/ensemble_inference) for further details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f9e770",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "Clone the repository, install the required packages, and import ensemble_inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb1eccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import pandas as pd\n",
    "import torch\n",
    "device = torch.device('cuda')\n",
    "\n",
    "!git clone https://github.com/jniimi/ensemble_inference\n",
    "%cd ensemble_inference\n",
    "!pip install -q -r requirements.txt\n",
    "import ensemble_inference as ens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89549b36",
   "metadata": {},
   "source": [
    "# Load Pretrained Model\n",
    "You need to register Hugging Face login token to `HF_TOKEN` in Colab Secret or manually login with huggingface_cli to retrieve the gated pretrained model like Llama from HF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf578f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model, tokenizer = ens.load_model(model_id=model_id, load_in_4bit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48275a3",
   "metadata": {},
   "source": [
    "## Create Prompt\n",
    "Enter the review text into `review_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d4a871",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {\n",
    "    'example_text': \"This restaurant is awesome!\",\n",
    "    'example_label': 5,\n",
    "    'review_text': \"The French restaurant I visited today was not very good because it took long to serve my dishes. But the food was not so bad. So, I will visit again.\"\n",
    "}\n",
    "prompt = ens.create_prompt(**input)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efc9bc4",
   "metadata": {},
   "source": [
    "# Single Inference\n",
    "Run single and non-ensemble inference. Specify `model_seed` (integer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d0cf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_seed = 1\n",
    "ens.single_inference(\n",
    "    prompt=prompt, \n",
    "    tokenizer=tokenizer, model=model, \n",
    "    model_seed=model_seed, model_temperature=1.0, \n",
    "    device=device\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84933c4",
   "metadata": {},
   "source": [
    "## Ensemble Inference\n",
    "Run multiple inferences with the specified random seeds and ensemble them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98274cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_seeds = [1, 2, 3, 4, 5]\n",
    "ens.ensemble_inference(\n",
    "    prompt=prompt, \n",
    "    tokenizer=tokenizer, model=model, \n",
    "    model_seeds=model_seeds, \n",
    "    model_temperature=1.0, \n",
    "    device=device\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
